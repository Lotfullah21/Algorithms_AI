{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMemV6TFVq6L2j4W4A9q6K4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lotfullah21/Algorithms_AI/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yvhiUOBJscNH"
      },
      "outputs": [],
      "source": [
        "# set some inputs\n",
        "x = -2; y = 5; z = -4\n",
        "\n",
        "# perform the forward pass\n",
        "q = x + y # q becomes 3\n",
        "f = q * z # f becomes -12\n",
        "\n",
        "# perform the backward pass (backpropagation) in reverse order:\n",
        "# first backprop through f = q * z\n",
        "dfdz = q # df/dz = q, so gradient on z becomes 3\n",
        "dfdq = z # df/dq = z, so gradient on q becomes -4\n",
        "dqdx = 1.0\n",
        "dqdy = 1.0\n",
        "# now backprop through q = x + y\n",
        "dfdx = dfdq * dqdx  # The multiplication here is the chain rule!\n",
        "dfdy = dfdq * dqdy "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "w = [2,-3,-3] # assume some random weights and data\n",
        "x = [-1, -2]\n",
        "\n",
        "# forward pass\n",
        "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
        "f = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n",
        "\n",
        "# backward pass through the neuron (backpropagation)\n",
        "ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\n",
        "dx = [w[0] * ddot, w[1] * ddot] # backprop into x\n",
        "dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w\n",
        "# we're done! we have the gradients on the inputs to the circuit"
      ],
      "metadata": {
        "id": "RGYG-RHFsveo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 3 # example values\n",
        "y = -4\n",
        "\n",
        "# forward pass\n",
        "sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)\n",
        "num = x + sigy # numerator                               #(2)\n",
        "sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)\n",
        "xpy = x + y                                              #(4)\n",
        "xpysqr = xpy**2                                          #(5)\n",
        "den = sigx + xpysqr # denominator                        #(6)\n",
        "invden = 1.0 / den                                       #(7)\n",
        "f = num * invden # done!                                 #(8)"
      ],
      "metadata": {
        "id": "3Lzt4djPs0MV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backprop f = num * invden\n",
        "dnum = invden # gradient on numerator                             #(8)\n",
        "dinvden = num                                                     #(8)\n",
        "# backprop invden = 1.0 / den \n",
        "dden = (-1.0 / (den**2)) * dinvden                                #(7)\n",
        "# backprop den = sigx + xpysqr\n",
        "dsigx = (1) * dden                                                #(6)\n",
        "dxpysqr = (1) * dden                                              #(6)\n",
        "# backprop xpysqr = xpy**2\n",
        "dxpy = (2 * xpy) * dxpysqr                                        #(5)\n",
        "# backprop xpy = x + y\n",
        "dx = (1) * dxpy                                                   #(4)\n",
        "dy = (1) * dxpy                                                   #(4)\n",
        "# backprop sigx = 1.0 / (1 + math.exp(-x))\n",
        "dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)\n",
        "# backprop num = x + sigy\n",
        "dx += (1) * dnum                                                  #(2)\n",
        "dsigy = (1) * dnum                                                #(2)\n",
        "# backprop sigy = 1.0 / (1 + math.exp(-y))\n",
        "dy += ((1 - sigy) * sigy) * dsigy                                 #(1)\n",
        "# done! phew"
      ],
      "metadata": {
        "id": "nqcsf9KNs5zf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# forward pass\n",
        "W = np.random.randn(5, 10)\n",
        "X = np.random.randn(10, 3)\n",
        "D = W.dot(X)\n",
        "\n",
        "# now suppose we had the gradient on D from above in the circuit\n",
        "dD = np.random.randn(*D.shape) # same shape as D\n",
        "dW = dD.dot(X.T) #.T gives the transpose of the matrix\n",
        "dX = W.T.dot(dD)"
      ],
      "metadata": {
        "id": "i70FynBqtBK0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dzdHxOYWtDWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}